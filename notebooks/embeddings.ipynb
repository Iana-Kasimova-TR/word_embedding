import sys
sys.path.append('../src/module')
from EmbedTransformer import EmbedTransformer
import TextProcessor
import Utils

import zipfile
import requests
from io import StringIO
import pandas as pd
import numpy as np
import nltk
import math
import sklearn
import csv
import string
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import word_tokenize
nltk.download('punkt')]


df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/posts.csv')

df_t = df[['clean_title', 'clean_tags']]
df_t.dropna(inplace=True)

df_t = df_t[df_t['clean_tags'].map(lambda x: len(x)) > 0]
df_t = df_t[df_t['clean_title'].map(lambda x: len(x)) > 0]
df_t.reset_index(drop=True, inplace=True)

f_t['clean_title'] = df_t['clean_title'].apply(lambda x:TextProcessor.remove_punctuation(x))
df_t['clean_tags'] = df_t['clean_tags'].apply(lambda x:TextProcessor.replace_under_score_on_whitespace(x))
df_t['clean_tags'] = df_t['clean_tags'].apply(lambda x:TextProcessor.remove_punctuation(x))
df_t['clean_tags'] = df_t['clean_tags'].apply(lambda x:x.split())

tag_list = [item for sublist in df_t['clean_tags'] for item in sublist]
text_tokens = [word_tokenize(title) for title in df_t['clean_title']]
text_list = [item for sublist in text_tokens for item in sublist]

tag_tokens_freq_dict = calculate_frequency_in_list(tag_list)
title_tokens_freq_dict = calculate_frequency_in_list(text_list)

tag_tokens_freq_dict.update(title_tokens_freq_dict)

# tokens have frequency more than 100 occurences
frequent_tokens = dict((k, v) for k, v in tag_tokens_freq_dict.items() if v>=100)
tokens_sorted_limit_list = list(frequent_tokens.keys())
tokens_dict = {i:tokens_sorted_limit_list[i] for i in range(len(tokens_sorted_limit_list))}
tokens_dict_reverse = {value:key for (key,value) in tokens_dict.items()}


# Declaring rows
N = len(tokens_dict_reverse.keys()) + 1
# Declaring columns
M = 300

matrix_embed = pd.DataFrame([np.random.normal(0, 1, M)] * N)


N_G = len(tokens_dict_reverse.keys()) + 1
M_G = 300
matrix_gradient = pd.DataFrame([np.array([1e-6 for i in range(M_G)])] * N_G)


df_t = df_t[df_t['clean_tags'].map(lambda x: len(x)) > 0]
df_t = df_t[df_t['clean_title'].map(lambda x: len(x)) > 0]
df_t.reset_index(drop=True, inplace=True)



df_t, df_t_test = train_test_split(df_t, test_size=0.2)


# training
epoch = 1
learning_rate = 0.01
dim = M
transformer = new EmbedTransformer
for i in range(epoch):
    matrix_title_tags, indexes = transformer.calculate_triplet(df_t, tokens_dict_reverse, matrix_embed)
    for ((anchor, correct, wrong), index_list) in zip(matrix_title_tags, indexes):
        loss = transformer.calculate_triplet_loss(anchor, correct, wrong)
        #matrix_title_tags = pd.DataFrame(matrix_title_tags)
        if loss > 0:
            anchor_grad, correct_grad, wrong_grad = transformer.calculate_gradient(anchor, correct, wrong)
            matrix_gradient = transformer.update_gradient_matrix_adagrad(matrix_gradient, index_list[0], anchor_grad)
            matrix_gradient = transformer.update_gradient_matrix_adagrad(matrix_gradient, index_list[1], correct_grad)
            matrix_gradient = transformer.update_gradient_matrix_adagrad(matrix_gradient, index_list[2], wrong_grad)
            matrix_embed = transformer.update_embed_matrix(matrix_embed, index_list[0], anchor_grad, matrix_gradient, learning_rate)
            matrix_embed = transformer.update_embed_matrix(matrix_embed, index_list[1], correct_grad, matrix_gradient, learning_rate)
            matrix_embed = transformer.update_embed_matrix(matrix_embed, index_list[2], wrong_grad, matrix_gradient, learning_rate)



# calculate matrix of titles and tags for test
M = 300
matrix_title = []
matrix_tags = []
for title, tags in zip(df_t_test['clean_title'], df_t_test['clean_tags']):
    title_index_list = Utils.get_indexes_of_tokens(word_tokenize(title),tokens_dict_reverse)
    tags_index_list = Utils.get_indexes_of_tokens(tags,tokens_dict_reverse)
    if(len(title_index_list) > 0 and len(tags_index_list) > 0):
        matrix_title.append(get_average_vector(matrix_embed, title_index_list, M))
        matrix_tags.append(get_average_vector(matrix_embed, tags_index_list, M))


matrix_title = np.matrix(matrix_title)
matrix_tags = np.matrix(matrix_tags)

transformer.calculate_map_metric(matrix_title, matrix_tags)

